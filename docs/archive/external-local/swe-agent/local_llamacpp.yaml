# SWE-agent config for llama.cpp server
# Usage: sweagent run-batch --config config/default.yaml --config config/local_llamacpp.yaml ...

agent:
  model:
    name: openai/local-model  # llama.cpp doesn't care about model name
    api_base: http://localhost:8090/v1
    api_key: not-needed
    per_instance_cost_limit: 0
    per_instance_call_limit: 50
    temperature: 0.0
    max_input_tokens: 32768
    max_output_tokens: 4096
