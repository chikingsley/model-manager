#!/bin/bash
# vLLM Control Script
# Usage: ./vllm-ctl [command] [options]

set -e
cd "$(dirname "$0")"

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

show_help() {
    echo "vLLM Control Script"
    echo ""
    echo "Usage: ./vllm-ctl [command]"
    echo ""
    echo "Commands:"
    echo "  start           Start vLLM server"
    echo "  stop            Stop vLLM server"
    echo "  restart         Restart vLLM server"
    echo "  logs            Show logs (follow mode)"
    echo "  status          Show server status"
    echo "  switch <model>  Switch to a different model"
    echo "  models          List available model presets"
    echo "  test            Test the API endpoint"
    echo "  shell           Open shell in container"
    echo ""
    echo "Examples:"
    echo "  ./vllm-ctl start"
    echo "  ./vllm-ctl switch qwen3-8b"
    echo "  ./vllm-ctl logs"
}

list_models() {
    echo -e "${GREEN}Available model presets:${NC}"
    echo ""
    echo "  ministral-8b    mistralai/Ministral-8B-Instruct-2410 (recommended)"
    echo "  qwen3-8b        Qwen/Qwen3-8B"
    echo "  qwen3-14b       Qwen/Qwen3-14B (needs quantization)"
    echo "  llama3-8b       meta-llama/Llama-3.1-8B-Instruct (needs HF token)"
    echo "  gemma2-9b       google/gemma-2-9b-it"
    echo ""
    echo "Or specify any HuggingFace model ID directly:"
    echo "  ./vllm-ctl switch TheBloke/some-model-AWQ"
}

switch_model() {
    local preset=$1
    local model=""
    local quant=""
    local extra=""

    case $preset in
        ministral-8b|ministral)
            model="mistralai/Ministral-8B-Instruct-2410"
            ;;
        qwen3-8b|qwen-8b|qwen8)
            model="Qwen/Qwen3-8B"
            ;;
        qwen3-14b|qwen-14b|qwen14)
            model="Qwen/Qwen3-14B"
            quant="fp8"
            echo -e "${YELLOW}Note: 14B model - enabling fp8 quantization${NC}"
            ;;
        llama3-8b|llama-8b|llama8)
            model="meta-llama/Llama-3.1-8B-Instruct"
            if [ -z "$HF_TOKEN" ] && ! grep -q "^HF_TOKEN=." .env; then
                echo -e "${RED}Warning: Llama requires HF_TOKEN in .env${NC}"
            fi
            ;;
        gemma2-9b|gemma-9b|gemma9)
            model="google/gemma-2-9b-it"
            ;;
        *)
            # Assume it's a direct model ID
            model="$preset"
            ;;
    esac

    if [ -z "$model" ]; then
        echo -e "${RED}Unknown model preset: $preset${NC}"
        list_models
        exit 1
    fi

    echo -e "${GREEN}Switching to: $model${NC}"

    # Update .env file
    sed -i "s|^MODEL=.*|MODEL=$model|" .env

    if [ -n "$quant" ]; then
        sed -i "s|^QUANTIZATION=.*|QUANTIZATION=$quant|" .env
    else
        sed -i "s|^QUANTIZATION=.*|QUANTIZATION=|" .env
    fi

    echo "Restarting container..."
    docker compose up -d --force-recreate vllm

    echo ""
    echo -e "${GREEN}Model switched! Monitor startup with:${NC}"
    echo "  ./vllm-ctl logs"
}

test_api() {
    local port=$(grep "^VLLM_PORT=" .env | cut -d= -f2)
    port=${port:-8000}

    echo "Testing vLLM API at localhost:$port..."
    echo ""

    # Health check
    if curl -sf "http://localhost:$port/health" > /dev/null 2>&1; then
        echo -e "${GREEN}Health check: OK${NC}"
    else
        echo -e "${RED}Health check: FAILED (server may still be loading)${NC}"
        exit 1
    fi

    # Model info
    echo ""
    echo "Available models:"
    curl -s "http://localhost:$port/v1/models" | python3 -m json.tool 2>/dev/null || \
        curl -s "http://localhost:$port/v1/models"

    # Quick completion test
    echo ""
    echo "Testing completion..."
    curl -s "http://localhost:$port/v1/chat/completions" \
        -H "Content-Type: application/json" \
        -d '{
            "model": "default",
            "messages": [{"role": "user", "content": "Say hello in exactly 3 words."}],
            "max_tokens": 20
        }' | python3 -m json.tool 2>/dev/null || echo "(raw response above)"
}

show_status() {
    echo "=== vLLM Status ==="
    docker compose ps
    echo ""

    local port=$(grep "^VLLM_PORT=" .env | cut -d= -f2)
    port=${port:-8000}

    if curl -sf "http://localhost:$port/health" > /dev/null 2>&1; then
        echo -e "API Status: ${GREEN}READY${NC}"
        echo ""
        echo "Current model:"
        curl -s "http://localhost:$port/v1/models" 2>/dev/null | \
            python3 -c "import sys,json; d=json.load(sys.stdin); print('  ' + d['data'][0]['id'])" 2>/dev/null || \
            echo "  (unable to fetch)"
    else
        echo -e "API Status: ${YELLOW}NOT READY${NC} (loading or stopped)"
    fi
}

# Main command handler
case "${1:-help}" in
    start)
        docker compose up -d
        echo -e "${GREEN}vLLM starting...${NC}"
        echo "First run takes 10-15 min to build. Monitor with: ./vllm-ctl logs"
        ;;
    stop)
        docker compose down
        echo -e "${GREEN}vLLM stopped${NC}"
        ;;
    restart)
        docker compose restart vllm
        echo -e "${GREEN}vLLM restarting...${NC}"
        ;;
    logs)
        docker compose logs -f vllm
        ;;
    status)
        show_status
        ;;
    switch)
        if [ -z "$2" ]; then
            echo -e "${RED}Usage: ./vllm-ctl switch <model>${NC}"
            list_models
            exit 1
        fi
        switch_model "$2"
        ;;
    models)
        list_models
        ;;
    test)
        test_api
        ;;
    shell)
        docker compose exec vllm /bin/bash
        ;;
    help|--help|-h)
        show_help
        ;;
    *)
        echo -e "${RED}Unknown command: $1${NC}"
        show_help
        exit 1
        ;;
esac
